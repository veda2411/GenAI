{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14608774",
   "metadata": {},
   "source": [
    "## Using pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2b17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172746b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", framework=\"tf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b57bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = sentiment_pipeline([\"I love my college!\",'I hate people opinion about AI'])\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572c9f33",
   "metadata": {},
   "source": [
    "piplines options - https://huggingface.co/docs/transformers/en/main_classes/pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f9164",
   "metadata": {},
   "source": [
    "Changing the default model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f217647b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline2 = pipeline(\"sentiment-analysis\", model=<model_from_hugging_face>, framework=\"tf\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0276318",
   "metadata": {},
   "source": [
    "## What if there is no pipeline and i want to use a different tokenizer and model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7376cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e423ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61404987",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-multilingual-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a2c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I love learning about NLP!\"\n",
    "tokens = tokenizer(text, return_tensors=\"tf\", padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c125f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokenized Words:\", tokenizer.tokenize(text))\n",
    "print(\"Token IDs:\", tokens[\"input_ids\"].numpy().tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98c5722",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211790c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = outputs.logits\n",
    "probs = tf.nn.softmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfd66a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_idx = tf.argmax(probs, axis=-1).numpy()[0]\n",
    "labels = [\"NEGATIVE\", \"POSITIVE\"]\n",
    "print(f\"Predicted Sentiment: {labels[label_idx]}, Probability: {probs.numpy().max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f67129c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16385b77",
   "metadata": {},
   "source": [
    "## Other examples of pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81224ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "prompt = \"There once lived a king\"\n",
    "result = generator(prompt, max_length=30, num_return_sequences=1)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3177e2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\")\n",
    "\n",
    "context = \"\"\"The Great Wall of China is a historic fortification that stretches \n",
    "over 13,000 miles. It was primarily built to protect against invasions and \n",
    "was constructed during the Ming Dynasty.\"\"\"\n",
    "\n",
    "question = \"Who built the Great Wall of China?\"\n",
    "\n",
    "result = qa_pipeline(question=question, context=context)\n",
    "\n",
    "print(result)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
