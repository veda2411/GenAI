{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b60aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install blis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc64e3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5216183b",
   "metadata": {},
   "source": [
    "### Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b84969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "\n",
    "text = \"Hello! How are you? I am learning NLP. Welcome to U.S.A. \"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "\n",
    "print(\"Word Tokens:\", word_tokens)\n",
    "print(\"Sentence Tokens:\", sent_tokens)\n",
    "print(\"SpaCy Tokens:\", spacy_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5131cd7d",
   "metadata": {},
   "source": [
    "### Stop word removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6af739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_stopwords = [token.text for token in doc if not token.is_stop]\n",
    "\n",
    "print(\"SpaCy Filtered Words:\", spacy_stopwords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a93168",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb1a670",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words = [token.lemma_ for token in doc]\n",
    "\n",
    "print(\"Lemmatized Words:\", lemmatized_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd89318",
   "metadata": {},
   "source": [
    "### Lower casr and Remove Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef527c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello!! This is NLP 101. Visit https://example.com\"\n",
    "cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove non-alphabetic characters\n",
    "lower_text = cleaned_text.lower()\n",
    "\n",
    "print(\"Cleaned Text:\", lower_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0ac363",
   "metadata": {},
   "source": [
    "### parts of Speech "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d27f705",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token.text} --> {token.pos_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b498637f",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4946e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text} --> {ent.label_}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e33063",
   "metadata": {},
   "source": [
    "### Word2Vec Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eee615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "\n",
    "model = Word2Vec(sentences=common_texts, vector_size=2, window=5, min_count=1, workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e4013",
   "metadata": {},
   "source": [
    "#### Embeddings for a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e71c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vector for 'computer':\", model.wv['computer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8624bb",
   "metadata": {},
   "source": [
    "#### Most similar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d155c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Most similar words to 'computer':\", model.wv.most_similar('eps'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d2f1fc",
   "metadata": {},
   "source": [
    "#### Visualizing closest words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "word_vectors = model.wv[model.wv.index_to_key]  # Get the word vectors\n",
    "pca = PCA(n_components=2)  # Initialize PCA\n",
    "result = pca.fit_transform(word_vectors)  # Fit and transform the word vectors\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "\n",
    "                \n",
    "words = list(model.wv.index_to_key)\n",
    "for i, word in enumerate(words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]), fontsize=12)\n",
    "\n",
    "plt.title(\"Word Embeddings Visualization\")\n",
    "plt.xlabel(\"PCA Component 1\")\n",
    "plt.ylabel(\"PCA Component 2\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18faf036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
